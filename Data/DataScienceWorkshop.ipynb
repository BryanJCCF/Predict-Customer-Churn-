{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='319ce38c-200b-4e4b-91bf-4a46a0c8c70d', project_access_token='p-fb744bb2ce502dd3534deab78d7295226faa9438')\npc = project.project_context\n", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Predecir el abandono de clientes (Churn) de una Telco utilizando SparkML en IBM Cloud Pak for Data (ICP4D)"}, {"metadata": {}, "cell_type": "markdown", "source": "Usaremos este cuaderno para crear un modelo de aprendizaje autom\u00e1tico para predecir el abandono de clientes. En este cuaderno, crearemos el modelo de predicci\u00f3n utilizando la biblioteca SparkML.\n\nEste cuaderno lo gu\u00eda a trav\u00e9s de estos pasos:\n\n- Cargar y visualizar un data set. (https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv)\n- Crear un modelo predictivo con SparkML API\n- Guardar el modelo en un repositorio de Ml"}, {"metadata": {}, "cell_type": "code", "source": "\n# El token del proyecto es un token de autorizaci\u00f3n que se utiliza para acceder a los recursos del proyecto, como fuentes de datos, conexiones y que utilizan las API de la plataforma.\n# Genere el TOKEN en la sesi\u00f3n de configuraci\u00f3n e inserte aqu\u00ed el c\u00f3digo usando el men\u00fa de arriba (3 puntos) \"Insertar Token del Proyecto\"\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.0 Instalar los paquetes requeridos\n\nHay un par de paquetes de Python que usaremos en este cuaderno. Primero nos aseguramos de que se elimine el cliente Watson Machine Learning v3 (no est\u00e1 instalado por defecto) y luego instalamos / actualizamos la versi\u00f3n v4 del cliente (este paquete se instala por defecto en CP4D).\n\nWML Client: https://wml-api-pyclient-dev-v4.mybluemix.net/#repository"}, {"metadata": {}, "cell_type": "code", "source": "!pip uninstall --yes watson-machine-learning-client-V4\n!pip install --user watson-machine-learning-client-V4\n!pip install --user pyspark==2.4 --upgrade|tail -n 1\n!pip install --user scikit-learn==0.20.3 --upgrade|tail -n 1", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nFound existing installation: watson-machine-learning-client-V4 1.0.103\nUninstalling watson-machine-learning-client-V4-1.0.103:\n  Successfully uninstalled watson-machine-learning-client-V4-1.0.103\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting watson-machine-learning-client-V4\n  Downloading watson_machine_learning_client_V4-1.0.135-py3-none-any.whl (1.3 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 18.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: urllib3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client-V4) (1.25.9)\nRequirement already satisfied: pandas<=1.0.5 in /home/wsuser/.local/lib/python3.7/site-packages (from watson-machine-learning-client-V4) (0.25.3)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client-V4) (2.24.0)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client-V4) (0.3.3)\nProcessing /tmp/wsuser/.cache/pip/wheels/47/22/bf/e1154ff0f5de93cc477acd0ca69abfbb8b799c5b28a66b44c2/ibm_cos_sdk-2.7.0-py2.py3-none-any.whl\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client-V4) (0.8.3)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from watson-machine-learning-client-V4) (2021.5.30)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas<=1.0.5->watson-machine-learning-client-V4) (1.18.5)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas<=1.0.5->watson-machine-learning-client-V4) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas<=1.0.5->watson-machine-learning-client-V4) (2020.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->watson-machine-learning-client-V4) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->watson-machine-learning-client-V4) (3.0.4)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from lomond->watson-machine-learning-client-V4) (1.15.0)\nProcessing /tmp/wsuser/.cache/pip/wheels/6c/a2/e4/c16d02f809a3ea998e17cfd02c13369281f3d232aaf5902c19/ibm_cos_sdk_core-2.7.0-py2.py3-none-any.whl\nProcessing /tmp/wsuser/.cache/pip/wheels/5f/b7/14/fbe02bc1ef1af890650c7e51743d1c83890852e598d164b9da/ibm_cos_sdk_s3transfer-2.7.0-py2.py3-none-any.whl\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ibm-cos-sdk==2.7.*->watson-machine-learning-client-V4) (0.9.4)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from ibm-cos-sdk-core==2.7.0->ibm-cos-sdk==2.7.*->watson-machine-learning-client-V4) (0.15.2)\nInstalling collected packages: ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk, watson-machine-learning-client-V4\n  Attempting uninstall: ibm-cos-sdk-core\n    Found existing installation: ibm-cos-sdk-core 2.6.0\n    Uninstalling ibm-cos-sdk-core-2.6.0:\n      Successfully uninstalled ibm-cos-sdk-core-2.6.0\n  Attempting uninstall: ibm-cos-sdk-s3transfer\n    Found existing installation: ibm-cos-sdk-s3transfer 2.6.0\n    Uninstalling ibm-cos-sdk-s3transfer-2.6.0:\n      Successfully uninstalled ibm-cos-sdk-s3transfer-2.6.0\n  Attempting uninstall: ibm-cos-sdk\n    Found existing installation: ibm-cos-sdk 2.6.0\n    Uninstalling ibm-cos-sdk-2.6.0:\n      Successfully uninstalled ibm-cos-sdk-2.6.0\nSuccessfully installed ibm-cos-sdk-2.7.0 ibm-cos-sdk-core-2.7.0 ibm-cos-sdk-s3transfer-2.7.0 watson-machine-learning-client-V4-1.0.135\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nSuccessfully installed pyspark-2.4.0\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.20.3) (1.18.5)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport json\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.0 Carga y limpia de datos\n\nVamos a cargar nuestros datos como un dataframe de Pandas.\n\n**<font color='red'><< SIGUE LAS INSTRUCCIONES DE ABAJO PARA CARGAR EL DATASET >></font>**\n\n* Resalte la celda de abajo haciendo clic en ella.\n* Haga clic en el icono \"Buscar datos\" `10/01` en la esquina superior derecha del cuaderno.\n* Si est\u00e1 utilizando datos virtualizados, comience eligiendo la pesta\u00f1a `Archivos`. Luego, elija sus datos virtualizados (es decir, MYSCHEMA.BILLINGPRODUCTCUSTOMERS), haga clic en `Insertar en el c\u00f3digo` y elija` Insertar Pandas DataFrame`.\n* Si est\u00e1 utilizando este cuaderno sin datos virtualizados, agregue el archivo cargado localmente `Telco-Customer-Churn.csv` eligiendo la pesta\u00f1a` Archivos`. Luego elija el `Telco-Customer-Churn.csv`. Haga clic en `Insertar en el c\u00f3digo` y elija` Insertar Pandas DataFrame`.\n* El c\u00f3digo para llevar los datos al entorno del port\u00e1til y crear un Pandas DataFrame se agregar\u00e1 a la celda a continuaci\u00f3n.\n* Ejecute la celda"}, {"metadata": {}, "cell_type": "code", "source": "# Resalta esta celda e ingresa el DataFrame de Pandas para los datos de los clientes\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# Inserta el c\u00f3digo de pandas abajo de esta l\u00ednea.\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Usaremos la convenci\u00f3n de nomenclatura de Pandas \"df\" para nuestro DataFrame. Aseg\u00farese de que la celda de abajo use el nombre del marco de datos usado arriba. Para el archivo cargado localmente, deber\u00eda verse como df_data_1 o df_data_2 o df_data_x. Para el caso de datos virtualizados, deber\u00eda verse como data_df_1 o data_df_2 o data_df_x.\n\n**<font color='red'><< ACTUALIZAR LA ASIGNACI\u00d3N DE VARIABLE A LA VARIABLE GENERADA ANTERIORMENTE. >></font>**"}, {"metadata": {}, "cell_type": "code", "source": "# for virtualized data\n# df = data_df_1\n\n# for local upload\ndf = df_data_2", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Eliminar la caracter\u00edstica CustomerID (columna)"}, {"metadata": {}, "cell_type": "code", "source": "df = df.drop('customerID', axis=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Examina los tipos de datos de las caracter\u00edsticas"}, {"metadata": {}, "cell_type": "code", "source": "df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Estad\u00edsticas de las columnas (caracter\u00edsticas). Config\u00farelo en todos, ya que el valor predeterminado es describir solo las funciones num\u00e9ricas.\ndf.describe(include = 'all')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Vemos que la tenencia var\u00eda de 0 (nuevo cliente) a 6 a\u00f1os, los cargos mensuales var\u00edan de $ 18 a $ 118, etc."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Verifique la necesidad de convertir la columna TotalCharges a num\u00e9rico si esta es detectada como un objeto.\n\nSi el `df.info` anterior muestra la columna\" TotalCharges \"como un objeto, necesitaremos convertirlo a num\u00e9rico. Si ya hizo esto durante un ejercicio anterior de \"Visualizaci\u00f3n de datos con refiner\u00eda de datos\", puede saltar al paso `2.4`."}, {"metadata": {}, "cell_type": "code", "source": "totalCharges = df.columns.get_loc(\"TotalCharges\")\nnew_col = pd.to_numeric(df.iloc[:, totalCharges], errors='coerce')\ndf.iloc[:, totalCharges] = pd.Series(new_col)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Estad\u00edsticas de las columnas (caracter\u00edsticas). Config\u00farelo en todos, ya que el valor predeterminado es describir solo las funciones num\u00e9ricas.\ndf.describe(include = 'all')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Ahora vemos estad\u00edsticas para la funci\u00f3n `TotalCharges`."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.4 Cualquier valor NaN debe de ser removido para crear un modelo m\u00e1s preciso."}, {"metadata": {}, "cell_type": "code", "source": "# Verifique si tenemos valores de NaN y vea qu\u00e9 caracter\u00edsticas tienen valores faltantes que deben abordarse\nprint(df.isnull().values.any())\ndf.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Deber\u00edamos ver que a la columna `TotalCharges` le faltan valores. Hay varias formas de abordar este problema:\n\n- Eliminar registros con valores perdidos\n- Complete el valor faltante con una de las siguientes estrategias: Cero, Media de los valores de la columna, Valor aleatorio, etc.)."}, {"metadata": {}, "cell_type": "code", "source": "# Manejar valores perdidos para la columna nan_column (TotalCharges)\nfrom sklearn.impute import SimpleImputer\n\n# Encuentra el n\u00famero de columna para TotalCharges (comenzando en 0).\ntotal_charges_idx = df.columns.get_loc(\"TotalCharges\")\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\ndf.iloc[:, total_charges_idx] = imputer.fit_transform(df.iloc[:, total_charges_idx].values.reshape(-1, 1))\ndf.iloc[:, total_charges_idx] = pd.Series(df.iloc[:, total_charges_idx])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Validar que nos hemos hecho cargo de cualquier valor NaN\nprint(df.isnull().values.any())\ndf.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n### 2.5 Clasificar las caracter\u00edsticas.\n\nClasificaremos algunas de las columnas / caracter\u00edsticas en funci\u00f3n de si son valores categ\u00f3ricos o valores continuos (es decir, num\u00e9ricos). Usaremos esto en secciones posteriores para crear visualizaciones."}, {"metadata": {}, "cell_type": "code", "source": "columns_idx = np.s_[0:] # Fragmento de la primera fila(header) con todas las columnas.\nfirst_record_idx = np.s_[0] # \u00cdndice de el primer registro\n\nstring_fields = [type(fld) is str for fld in df.iloc[first_record_idx, columns_idx]] # Todos los campos tipo string\nall_features = [x for x in df.columns if x != 'Churn']\ncategorical_columns = list(np.array(df.columns)[columns_idx][string_fields])\ncategorical_features = [x for x in categorical_columns if x != 'Churn']\ncontinuous_features = [x for x in all_features if x not in categorical_features]\n\n#print('All Features: ', all_features)\n#print('\\nCategorical Features: ', categorical_features)\n#print('\\nContinuous Features: ', continuous_features)\n#print('\\nAll Categorical Columns: ', categorical_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.6 Visualizar los datos\n\nLa visualizaci\u00f3n de datos se puede utilizar para encontrar patrones, detectar valores at\u00edpicos, comprender la distribuci\u00f3n y m\u00e1s. Podemos utilizar gr\u00e1ficos como:\n\n- Histogramas, diagramas de caja, etc: Para encontrar la distribuci\u00f3n / propagaci\u00f3n de nuestras variables continuas.\n- Gr\u00e1ficos de barras: para mostrar la frecuencia en valores categ\u00f3ricos."}, {"metadata": {}, "cell_type": "code", "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Primero, obtenemos una vista de alto nivel de la distribuci\u00f3n de la \"Tasa de abandono (Churn)\". \u00bfQu\u00e9 porcentaje de clientes en nuestro conjunto de datos est\u00e1n agitando frente a los que no lo est\u00e1n?"}, {"metadata": {}, "cell_type": "code", "source": "print(df.groupby(['Churn']).size())\nchurn_plot = sns.countplot(data=df, x='Churn', order=df.Churn.value_counts().index)\nplt.ylabel('Count')\nfor p in churn_plot.patches:\n    height = p.get_height()\n    churn_plot.text(p.get_x()+p.get_width()/2., height + 1,'{0:.0%}'.format(height/float(len(df))),ha=\"center\") \nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Podemos utilizar gr\u00e1ficos de recuentos de frecuencia para comprender las caracter\u00edsticas categ\u00f3ricas relativas a la \"Tasa de abandono (Churn)\"\n\n- Podemos ver eso para la funci\u00f3n de g\u00e9nero. Tenemos tasas de abandono relativamente iguales por \"g\u00e9nero\"\n- Podemos ver eso para la funci\u00f3n `InternetService`. Tenemos una mayor tasa de abandono para los que tienen servicio de \"fibra \u00f3ptica\" en comparaci\u00f3n con los que tienen \"DSL\"."}, {"metadata": {}, "cell_type": "code", "source": "# Gr\u00e1ficos de recuento de caracter\u00edsticas categ\u00f3ricas.\nf, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9), (ax10, ax11, ax12), (ax13, ax14, ax15)) = plt.subplots(5, 3, figsize=(20, 20))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15 ]\n\nfor i in range(len(categorical_features)):\n    sns.countplot(x = categorical_features[i], hue=\"Churn\", data=df, ax=ax[i])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Podemos usar gr\u00e1ficos de histrograma para comprender la distribuci\u00f3n de nuestras caracter\u00edsticas continuas / num\u00e9ricas en relaci\u00f3n con el abandono.\n\n- Podemos ver que para la funci\u00f3n `MonthlyCharges`, los clientes que abandonan tienden a pagar tarifas mensuales m\u00e1s altas que los que se quedan.\n- Podemos ver que para la funci\u00f3n de tenencia, los clientes que abandonan tienden a ser clientes relativamente nuevos."}, {"metadata": {}, "cell_type": "code", "source": "# Histogramas de caracter\u00edsticas continuas.\nfig, ax = plt.subplots(2, 2, figsize=(28, 8))\ndf[df.Churn == 'No'][continuous_features].hist(bins=20, color=\"blue\", alpha=0.5, ax=ax)\ndf[df.Churn == 'Yes'][continuous_features].hist(bins=20, color=\"orange\", alpha=0.5, ax=ax)\n\n# O usa displots\n#sns.set_palette(\"hls\", 3)\n#f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(25, 25))\n#ax = [ax1, ax2, ax3, ax4]\n#for i in range(len(continuous_features)):\n#    sns.distplot(df[continuous_features[i]], bins=20, hist=True, ax=ax[i])", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Crea una cuadr\u00edcula para relaciones por pares\ngr = sns.PairGrid(df, height=5, hue=\"Churn\")\ngr = gr.map_diag(plt.hist)\ngr = gr.map_offdiag(plt.scatter)\ngr = gr.add_legend()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Trace diagramas de caja de columnas num\u00e9ricas. Una mayor variaci\u00f3n en la gr\u00e1fica de caja implica una mayor importancia.\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4]\n\nfor i in range(len(continuous_features)):\n    sns.boxplot(x = 'Churn', y = continuous_features[i], data=df, ax=ax[i])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3.0 Creaci\u00f3n de un modelo de ML\n\nAhora podemos crear nuestro modelo de aprendizaje autom\u00e1tico. Puede usar la informaci\u00f3n / intuici\u00f3n obtenida de los pasos de visualizaci\u00f3n de datos anteriores para saber qu\u00e9 tipo de modelo crear o qu\u00e9 funciones usar. Crearemos un modelo de clasificaci\u00f3n simple."}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport json\n\nspark = SparkSession.builder.getOrCreate()\ndf_data = spark.createDataFrame(df)\ndf_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1 Divida los datos en conjuntos de prueba y entrenamiento"}, {"metadata": {}, "cell_type": "code", "source": "spark_df = df_data\n(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n\nprint(\"N\u00famero de registros para el entrenamiento: \" + str(train_data.count()))\nprint(\"N\u00famero de registros para la evaluaci\u00f3n: \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2 Examina el esquema del DataFrame Spark\nObserve los tipos de datos para determinar los requisitos para la ingenier\u00eda de funciones."}, {"metadata": {}, "cell_type": "code", "source": "spark_df.printSchema()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Utiliza la funci\u00f3n StringIndexer para codificar una columna de cadenas de etiquetas en una columna de \u00edndices de etiquetas\n\nEstamos utilizando el paquete Pipeline para construir los pasos de desarrollo como una pipeline.\nEstamos usando StringIndexer para manejar caracter\u00edsticas categ\u00f3ricas / de cadena del conjunto de datos. StringIndexer codifica una columna de cadenas de etiquetas en una columna de \u00edndices de etiquetas\n\nLuego usamos VectorAssembler para ensamblar estas caracter\u00edsticas en un vector. La API de canalizaciones requiere que las variables de entrada se pasen en un vector"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\n\n\nsi_gender = StringIndexer(inputCol = 'gender', outputCol = 'gender_IX')\nsi_Partner = StringIndexer(inputCol = 'Partner', outputCol = 'Partner_IX')\nsi_Dependents = StringIndexer(inputCol = 'Dependents', outputCol = 'Dependents_IX')\nsi_PhoneService = StringIndexer(inputCol = 'PhoneService', outputCol = 'PhoneService_IX')\nsi_MultipleLines = StringIndexer(inputCol = 'MultipleLines', outputCol = 'MultipleLines_IX')\nsi_InternetService = StringIndexer(inputCol = 'InternetService', outputCol = 'InternetService_IX')\nsi_OnlineSecurity = StringIndexer(inputCol = 'OnlineSecurity', outputCol = 'OnlineSecurity_IX')\nsi_OnlineBackup = StringIndexer(inputCol = 'OnlineBackup', outputCol = 'OnlineBackup_IX')\nsi_DeviceProtection = StringIndexer(inputCol = 'DeviceProtection', outputCol = 'DeviceProtection_IX')\nsi_TechSupport = StringIndexer(inputCol = 'TechSupport', outputCol = 'TechSupport_IX')\nsi_StreamingTV = StringIndexer(inputCol = 'StreamingTV', outputCol = 'StreamingTV_IX')\nsi_StreamingMovies = StringIndexer(inputCol = 'StreamingMovies', outputCol = 'StreamingMovies_IX')\nsi_Contract = StringIndexer(inputCol = 'Contract', outputCol = 'Contract_IX')\nsi_PaperlessBilling = StringIndexer(inputCol = 'PaperlessBilling', outputCol = 'PaperlessBilling_IX')\nsi_PaymentMethod = StringIndexer(inputCol = 'PaymentMethod', outputCol = 'PaymentMethod_IX')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "si_Label = StringIndexer(inputCol=\"Churn\", outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.4 Creando un solo vector"}, {"metadata": {}, "cell_type": "code", "source": "va_features = VectorAssembler(inputCols=['gender_IX',  'SeniorCitizen', 'Partner_IX', 'Dependents_IX', 'PhoneService_IX', 'MultipleLines_IX', 'InternetService_IX', \\\n                                         'OnlineSecurity_IX', 'OnlineBackup_IX', 'DeviceProtection_IX', 'TechSupport_IX', 'StreamingTV_IX', 'StreamingMovies_IX', \\\n                                         'Contract_IX', 'PaperlessBilling_IX', 'PaymentMethod_IX', 'TotalCharges', 'MonthlyCharges'], outputCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.5 Crear una pipeline, y ajustar un modelo usando un algoritmo RandomForestClassifier \nRe\u00fana todas las etapas en una pipeline. No esperamos una regresi\u00f3n lineal limpia, por lo que usaremos RandomForestClassifier para encontrar el mejor \u00e1rbol de decisi\u00f3n para los datos."}, {"metadata": {}, "cell_type": "code", "source": "classifier = RandomForestClassifier(featuresCol=\"features\")\n\npipeline = Pipeline(stages=[si_gender, si_Partner, si_Dependents, si_PhoneService, si_MultipleLines, si_InternetService, si_OnlineSecurity, si_OnlineBackup, si_DeviceProtection, \\\n                            si_TechSupport, si_StreamingTV, si_StreamingMovies, si_Contract, si_PaperlessBilling, si_PaymentMethod, si_Label, va_features, \\\n                            classifier, label_converter])\n\nmodel = pipeline.fit(train_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "predictions = model.transform(test_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(predictions)\n\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderROC')\narea_under_curve = evaluatorDT.evaluate(predictions)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderPR')\narea_under_PR = evaluatorDT.evaluate(predictions)\nprint(\"areaUnderROC = %g\" % area_under_curve)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.0 Guarda el modelo y prueba los datos.\n\nAhora el modelo se puede guardar para una futura implementaci\u00f3n. El modelo se guardar\u00e1 utilizando el cliente Watson Machine Learning, en un espacio de implementaci\u00f3n.\n**<font color='red'><< ACTUALIZAR LA VARIABLE 'MODEL_NAME' A UN NOMBRE \u00daNICO>></font>**\n\n**<font color='red'><< ACTUALIZAR LA VARIABLE 'DEPLOYMENT_SPACE_NAME' AL NOMBRE DEL ESPACIO DE DESPLIEGUE CREADO ANTERIORMENTE >></font>**"}, {"metadata": {}, "cell_type": "code", "source": "\nMODEL_NAME = \"PREDICT-CHURN\"\nDEPLOYMENT_SPACE_NAME = 'Big-Data'\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Guarda el modelo en una instancia local de ICP4D Watson Machine Learning\n\n1. Genera una API Key: https://cloud.ibm.com/iam/apikeys\n2. Genera un TOKEN para tu instancia de Watson Machine Learning:\n   curl --insecure -X POST --header \"Content-Type: application/x-www-form-urlencoded\" --header \"Accept: application/json\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" --data-urlencode \"apikey=$API_key\" \"https://iam.ng.bluemix.net/identity/token\"\n3.   <font color='red'>Reemplaza el valor de `token` marcado con `*****` con el `token` generado. El valor de el campo `url` debe ser igual a el valor del campo `url` de tu instancia de Watson Machine Learning.</font>"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nwml_credentials = {\n                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n                   \"token\":\"*******\"\n}\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.spaces.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Utiliza el espacio deseado como el `default_space`\n\nEl ID del espacio de implementaci\u00f3n se buscar\u00e1 seg\u00fan el nombre especificado anteriormente. Si no recibe un GUID de espacio como resultado de la siguiente celda, no contin\u00fae hasta que haya creado un espacio de implementaci\u00f3n."}, {"metadata": {}, "cell_type": "code", "source": "# Aseg\u00farese de actualizar el nombre del espacio con el que desea utilizar.\n#client.spaces.list()\nall_spaces = client.spaces.get_details()['resources']\nspace_id = None\n#print(all_spaces)\nfor space in all_spaces:\n    if space['entity']['name'] == DEPLOYMENT_SPACE_NAME:\n        space_id = space[\"metadata\"][\"id\"]\n        print(\"\\nDeployment Space GUID: \", space_id)\n\nif space_id is None:\n    print(\"ADVERTENCIA: Tu espacio no existe. Cree un espacio de implementaci\u00f3n antes de pasar a la siguiente celda.\")\n    #space_id = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: space_name})[\"metadata\"][\"guid\"]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**<font color='red'><< REEMPLAZA space_id de ABAJO con el ID de tu espacio. Por ejemplo: <br/>client.set.default_space(\"6b39c537-f707-4078-9dc7-ce70b70ab22f\") >></font>**"}, {"metadata": {}, "cell_type": "code", "source": "# Ahora configure el espacio predeterminado en el GUID para su espacio de implementaci\u00f3n. Si tiene \u00e9xito, ver\u00e1 un mensaje de \"Success\".\nclient.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Guarda el Modelo"}, {"metadata": {}, "cell_type": "code", "source": "# En caso de que necesite verificar los servicios, descomente la l\u00ednea a continuaci\u00f3n y ejec\u00fatelo.\n#client.software_specifications.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "software_spec_id =  client.software_specifications.get_id_by_name('spark-mllib_2.4')\nprint(software_spec_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Guarda el modelo.\nmodel_props = {client.repository.ModelMetaNames.NAME: MODEL_NAME,\n               client.repository.ModelMetaNames.SOFTWARE_SPEC_UID : software_spec_id,\n               client.repository.ModelMetaNames.TYPE : \"mllib_2.4\"}\npublished_model = client.repository.store_model(model=model, pipeline=pipeline, meta_props=model_props, training_data=train_data)\n\nprint(json.dumps(published_model, indent=3))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Utilice esta celda para realizar cualquier limpieza de modelos e implementaciones previamente creados\nclient.repository.list_models()\nclient.deployments.list()\n\n# client.repository.delete('GUID de el modelo guardado')\n# client.deployments.delete('GUID de el modelo desplegado')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5.0 Guarda los datos de pruebas\n\nGuardaremos los datos de prueba que usamos para evaluar el modelo en nuestro proyecto."}, {"metadata": {}, "cell_type": "code", "source": "write_score_CSV=test_data.toPandas().drop(['Churn'], axis=1)\n#write_score_CSV.to_csv('/project_data/data_asset/TelcoCustomerSparkMLBatchScore.csv', sep=',', index=False)\nproject.save_data('TelcoCustomerSparkMLBatchScore.csv', write_score_CSV.to_csv(), overwrite=True)\n\nwrite_eval_CSV=test_data.toPandas()\n#write_eval_CSV.to_csv('/project_data/data_asset/TelcoCustomerSparkMLEval.csv', sep=',', index=False)\nproject.save_data('TelcoCustomerSparkMLEval.csv', write_eval_CSV.to_csv(), overwrite=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Felicidades!!! ha creado un modelo basado en datos de abandono de clientes y lo ha implementado en Watson Machine Learning."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}